### Multi-Part Upload

#### Process
##### Initiate Upload
- You send an "Initiate Multipart Upload" request to S3 for the specific object (bucket and key/filename).
-  S3 responds with a unique `UploadId`. This ID acts like a session token for this specific upload operation. You'll use it in all subsequent steps for this file.
##### Upload Parts
- You read your large file chunk by chunk (these are the "parts").
- S3 requires parts to be between 5MB and 5GB, except for the very last part, which can be smaller than 5MB. A common part size is 8MB-100MB.
- For each part, you send an "Upload Part" request to S3. This request includes:
    - The `UploadId` from step 1.
    - A unique `PartNumber` (sequentially numbered, starting from 1).
    - The actual data (bytes) for that specific part.
- S3 uploads the part and responds with an `ETag` (an entity tag, essentially a checksum/hash of the part's data).
- **Crucially, you must store both the `PartNumber` and its corresponding `ETag` for every successfully uploaded part.**
##### Complete Upload
- Once you have successfully uploaded _all_ the parts of the file:
- You send a "Complete Multipart Upload" request to S3. This request includes:
    - The `UploadId` from step 1.
    - A list containing all the `{PartNumber, ETag}` pairs you collected in step 2, **sorted in ascending order by PartNumber**.
- S3 verifies it has all the specified parts (using the ETags) and then assembles them server-side into the final object in the correct order.
- S3 confirms the object creation.
##### Abort Upload (Error Handling / Cleanup)
- If anything goes wrong during the process (network error on a part, user cancels, etc.), or if you decide not to complete the upload, you _must_ send an "Abort Multipart Upload" request using the `UploadId`.
- This tells S3 to delete all the parts that were uploaded for that specific `UploadId`.
- **Important:** If you initiate an upload and upload some parts but never complete or abort it, those uploaded parts will remain in your bucket (hidden from normal listings) and **you will be charged for their storage**. Always implement cleanup logic.
#### Benefits
- **Improved Throughput:** Parts can be uploaded in parallel (e.g., using multiple threads or async operations) to potentially speed up the total upload time.
- **Resilience:** If a single part fails to upload due to network issues, you only need to retry uploading that specific part, not the entire file.
- **Pause and Resume:** You can pause an upload by saving the `UploadId` and the list of successfully uploaded `{PartNumber, ETag}` pairs. You can resume later by initiating uploads for the remaining parts and then completing with the full list.
- **Large File Support:** Allows uploading files much larger than the 5GB single-operation limit (up to 5TB).
#### Prototype
https://github.com/vishalkotak/python-prototypes/blob/main/src/system-design-prototype/dropbox/s3_multi_part_upload.py
